{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------preprocess--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python 3.6.13\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import sys\n",
    "from collections import Counter\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.signal as sg\n",
    "import wfdb\n",
    "from tqdm import tqdm\n",
    "#install opencv-python==4.5.5.64 first then U can install biosppy\n",
    "import biosppy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 250\n",
    "invalid_labels = ['|', '~', '!', '+', '[', ']', '\"', 'x']  # non-beat labels\n",
    "before = 63\n",
    "after = 77\n",
    "tol = 0.05\n",
    "\n",
    "#write data link\n",
    "base_dir=\"C:/Users/ASUS/Desktop/cg working/HRV/羅孟宗老師 已標註波形/羅孟宗老師 已標註波形/\"\n",
    "ECGfile=[\"完成檔raw data/B257_2016_09_07-14_03_09_ECG.csv\",\\\n",
    "         \"完成檔raw data/B268_2016_12_13-14_34_26_ECG.csv\",\\\n",
    "         \"完成檔raw data/B268_2016_12_14-10_12_18_ECG.csv\",\\\n",
    "         \"完成檔raw data/B357#1_2017_09_12-11_10_13_ECG.csv\",\\\n",
    "         \"完成檔raw data/C727_2016_08_17-14_35_55_ECG.csv\",\\\n",
    "         \"完成檔raw data/C727_2016_08_18-14_15_50_ECG.csv\",\\\n",
    "         \"完成檔raw data/C727_2016_08_19-09_00_24_ECG.csv\",\\\n",
    "         \"完成檔raw data/C733_2016_08_24-09_23_45_ECG.csv\",\\\n",
    "         \"完成檔raw data/C733_2016_08_25-15_41_37_ECG.csv\",\\\n",
    "         \"完成檔raw data/C733_2016_08_26-11_50_31_ECG.csv\",\\\n",
    "         \"完成檔raw data/C783_2016_10_04-15_38_04_ECG.csv\",\\\n",
    "         \"完成檔raw data/C790_2016_10_28-09_36_26_ECG.csv\",\\\n",
    "         \"完成檔raw data/C790_2016_10_28-09_59_36_ECG.csv\",\\\n",
    "         \"完成檔raw data/C790_2016_10_28-10_54_43_ECG.csv\",\\\n",
    "         \"完成檔raw data/C790_2016_10_28-14_51_20_ECG.csv\",\\\n",
    "         \"完成檔raw data/C795_2016_11_02-11_43_05_ECG.csv\",\\\n",
    "         \"完成檔raw data/C795_2016_11_03-09_28_28_ECG.csv\",\\\n",
    "         \"完成檔raw data/C795_2016_11_03-14_28_24_ECG.csv\",\\\n",
    "         \"完成檔raw data/C795_2016_11_04-07_48_32_ECG.csv\",\\\n",
    "         \"完成檔raw data/C807_2016_11_16-11_46_27_ECG.csv\",\\\n",
    "         \"完成檔raw data/C807_2016_11_18-07_31_57_ECG.csv\",\\\n",
    "         \"完成檔raw data/C818_2016_11_29-14_52_43_ECG.csv\",\\\n",
    "         \"完成檔raw data/C818_2016_11_29-15_08_33_ECG.csv\",\\\n",
    "         \"完成檔raw data/P119#1_2017_08_30-10_46_36_ECG.csv\"]\n",
    "Markfile=[\"標註在raw data 上/B257_2016_09_07-14_03_09_True_rr.csv\",\\\n",
    "          \"標註在raw data 上/B268_2016_12_13-14_34_26_True_rr.csv\",\\\n",
    "          \"標註在raw data 上/B268_2016_12_14-10_12_18_True_rr.csv\",\\\n",
    "          \"標註在raw data 上/B357-1_2017_09_12-11_10_13_True_rr.csv\",\\\n",
    "          \"標註在raw data 上/C727_2016_08_17-14_35_55_True_rr.csv\",\\\n",
    "          \"標註在raw data 上/C727_2016_08_18-14_15_50_True_rr.csv\",\\\n",
    "          \"標註在raw data 上/C727_2016_08_19-09_00_24_True_rr.csv\",\\\n",
    "          \"標註在raw data 上/C733_2016_08_24-09_23_45_True_rr.csv\",\\\n",
    "          \"標註在raw data 上/C733_2016_08_25-15_41_37_True_rr.csv\",\\\n",
    "          \"標註在raw data 上/C733_2016_08_26-11_50_31_True_rr.csv\",\\\n",
    "          \"標註在raw data 上/C783_2016_10_04-15_38_04_True_rr.csv\",\\\n",
    "          \"標註在raw data 上/C790_2016_10_28-09_36_26_True_rr.csv\",\\\n",
    "          \"標註在raw data 上/C790_2016_10_28-09_59_36_True_rr.csv\",\\\n",
    "          \"標註在raw data 上/C790_2016_10_28-10_54_43_True_rr.csv\",\\\n",
    "          \"標註在raw data 上/C790_2016_10_28-14_51_20_True_rr.csv\",\\\n",
    "          \"標註在raw data 上/C795_2016_11_02-11_43_05_True_rr.csv\",\\\n",
    "          \"標註在raw data 上/C795_2016_11_03-09_28_28_True_rr.csv\",\\\n",
    "          \"標註在raw data 上/C795_2016_11_03-14_28_24_True_rr.csv\",\\\n",
    "          \"標註在raw data 上/C795_2016_11_04-07_48_32_True_rr.csv\",\\\n",
    "          \"標註在raw data 上/C807_2016_11_16-11_46_27_True_rr.csv\",\\\n",
    "          \"標註在raw data 上/C807_2016_11_18-07_31_57_True_rr.csv\",\\\n",
    "          \"標註在raw data 上/C818_2016_11_29-14_52_43_True_rr.csv\",\\\n",
    "          \"標註在raw data 上/C818_2016_11_29-15_08_33_True_rr.csv\",\\\n",
    "          \"標註在raw data 上/P119-1_2017_08_30-10_46_36_True_rr.csv\"]\n",
    "\n",
    "#write save link (after preprocess and we will get training data & testing data)\n",
    "link=\"C:/Users/ASUS/Desktop/cg working/HRV/Identify correct wave methods/Deep-Neural-Network-For-Heartbeat-Classification-master/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(EcgWaveform,Mark):\n",
    "    filename = EcgWaveform\n",
    "    signal = pd.read_csv(filename)[\"EcgWaveform\"]\n",
    "    signal=np.array(list(signal),dtype=\"f8\")\n",
    "\n",
    "    baseline = sg.medfilt(sg.medfilt(signal, int(0.2 * sampling_rate) - 1), int(0.6 * sampling_rate) - 1)\n",
    "    signal = signal - baseline\n",
    "\n",
    "    lab=pd.read_csv(Mark)\n",
    "    r_peaks=np.array(list(round(lab[\"position\"])),dtype=\"int\")\n",
    "    labels=np.array(list(lab[\"mark\"]))\n",
    "    \n",
    "    # align r-peaks\n",
    "    newR = []\n",
    "    for r_peak in r_peaks:\n",
    "        r_left = np.maximum(r_peak - int(tol * sampling_rate), 0)\n",
    "        r_right = np.minimum(r_peak + int(tol * sampling_rate), len(signal))\n",
    "        newR.append(r_left + np.argmax(signal[r_left:r_right]))\n",
    "    r_peaks = np.array(newR, dtype=\"int\")\n",
    "\n",
    "    # r-peaks intervals\n",
    "    rris = np.array(lab[\"RR\"])\n",
    "\n",
    "    avg_rri = np.mean(rris)\n",
    "    x1, x2, y = [], [], []\n",
    "    for index in tqdm(range(len(r_peaks)), file=sys.stdout):\n",
    "        if index == 0 or index == len(r_peaks) - 1:\n",
    "            continue\n",
    "        beat = signal[r_peaks[index] - before: r_peaks[index] + after]\n",
    "\n",
    "        pre_rri = rris[index - 1]\n",
    "        post_rri = rris[index]\n",
    "        ratio_rri = pre_rri / post_rri\n",
    "        local_rri = np.mean(rris[np.maximum(index - 10, 0):index])\n",
    "\n",
    "        if labels[index] in [\"N\", \"L\", \"R\", \"e\", \"j\"]:\n",
    "            label = 0  # N\n",
    "        elif labels[index] in [\"A\", \"a\", \"S\", \"J\"]:\n",
    "            label = 1  # SVEB\n",
    "        elif labels[index] in [\"V\", \"E\"]:\n",
    "            label = 1  # VEB\n",
    "        elif labels[index] in [\"F\"]:\n",
    "            label = 1  # F\n",
    "        elif labels[index] in [\"/\", \"f\", \"Q\"]:\n",
    "            label = 1  # Q\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        x1.append(beat)\n",
    "        x2.append([pre_rri - avg_rri, post_rri - avg_rri, ratio_rri, local_rri - avg_rri])\n",
    "        y.append(label)\n",
    "\n",
    "    return x1, x2, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3660/3660 [00:00<00:00, 109479.83it/s]\n",
      "100%|██████████| 2332/2332 [00:00<00:00, 143628.74it/s]\n",
      "100%|██████████| 2443/2443 [00:00<00:00, 142036.91it/s]\n",
      "100%|██████████| 7348/7348 [00:00<00:00, 73049.02it/s]\n",
      "100%|██████████| 2763/2763 [00:00<00:00, 106592.67it/s]\n",
      "100%|██████████| 1590/1590 [00:00<00:00, 178471.47it/s]\n",
      "100%|██████████| 1518/1518 [00:00<00:00, 95460.87it/s]\n",
      "100%|██████████| 3382/3382 [00:00<00:00, 103268.27it/s]\n",
      "100%|██████████| 2923/2923 [00:00<00:00, 112236.69it/s]\n",
      "100%|██████████| 2574/2574 [00:00<00:00, 151193.72it/s]\n",
      "100%|██████████| 2638/2638 [00:00<00:00, 160379.39it/s]\n",
      "100%|██████████| 1610/1610 [00:00<00:00, 61083.94it/s]\n",
      "100%|██████████| 2452/2452 [00:00<00:00, 112963.61it/s]\n",
      "100%|██████████| 1610/1610 [00:00<00:00, 82674.21it/s]\n",
      "100%|██████████| 4031/4031 [00:00<00:00, 110653.82it/s]\n",
      "100%|██████████| 2230/2230 [00:00<00:00, 130394.08it/s]\n",
      "100%|██████████| 2551/2551 [00:00<00:00, 76622.14it/s]\n",
      "100%|██████████| 2594/2594 [00:00<00:00, 81312.54it/s]\n",
      "100%|██████████| 3076/3076 [00:00<00:00, 76932.17it/s]\n",
      "100%|██████████| 1056/1056 [00:00<?, ?it/s]\n",
      "100%|██████████| 1921/1921 [00:00<00:00, 114795.38it/s]\n",
      "100%|██████████| 1733/1733 [00:00<00:00, 105581.07it/s]\n",
      "100%|██████████| 3702/3702 [00:00<00:00, 73888.92it/s]\n",
      "100%|██████████| 148/148 [00:00<?, ?it/s]\n",
      "train labels: Counter({0: 49120, 1: 339})\n",
      "test labels: Counter({0: 12304, 1: 74})\n"
     ]
    }
   ],
   "source": [
    "x1_train, x2_train, y_train = [], [], []\n",
    "x1_test, x2_test, y_test = [], [], []\n",
    "np.random.seed(1225)\n",
    "for i in range(len(ECGfile)):\n",
    "    x1,x2,y=pre_processing(os.path.join(base_dir, ECGfile[i]),os.path.join(base_dir, Markfile[i]))\n",
    "    a=np.floor(0.8*len(y))\n",
    "    if (0.8*len(y)-a)>=0.5:\n",
    "        a+1\n",
    "    b=len(y)-a\n",
    "    a=int(a)\n",
    "    b=int(b)\n",
    "    rdgroup=np.random.choice([0]*a+[1]*b,len(y),replace=False)\n",
    "\n",
    "    x1tn,x2tn,ytn,x1tt,x2tt,ytt=[],[],[],[],[],[]\n",
    "    for i in range(len(y)):\n",
    "        if rdgroup[i]==1:\n",
    "            x1tt.append(x1[i])\n",
    "            x2tt.append(x2[i])\n",
    "            ytt.append(y[i])\n",
    "        if rdgroup[i]==0:\n",
    "            x1tn.append(x1[i])\n",
    "            x2tn.append(x2[i])\n",
    "            ytn.append(y[i])\n",
    "    \n",
    "    x1_train.append(x1tn)\n",
    "    x2_train.append(x2tn)\n",
    "    y_train.append(ytn)\n",
    "    x1_test.append(x1tt)\n",
    "    x2_test.append(x2tt)\n",
    "    y_test.append(ytt)\n",
    "\n",
    "x1_train = np.concatenate(x1_train, axis=0)\n",
    "x2_train = np.concatenate(x2_train, axis=0)\n",
    "y_train = np.concatenate(y_train, axis=0)\n",
    "x1_test = np.concatenate(x1_test, axis=0)\n",
    "x2_test = np.concatenate(x2_test, axis=0)\n",
    "y_test = np.concatenate(y_test, axis=0)\n",
    "\n",
    "with open(os.path.join(link, \"CNNMPFL_preprocess.pkl\"), \"wb\") as f:\n",
    "        pickle.dump((\n",
    "            (x1_train, x2_train, y_train),\n",
    "            (x1_test, x2_test, y_test)\n",
    "        ), f, protocol=4)\n",
    "\n",
    "print(\"train labels:\", Counter(y_train))\n",
    "print(\"test labels:\", Counter(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------training--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "from keras import backend as K\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.layers import Activation, BatchNormalization, Concatenate, Conv1D, Dense, Flatten, Input, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_classes\n",
    "p=2\n",
    "#shape\n",
    "shp=140\n",
    "#wtraining data & testing data link\n",
    "link2=link\n",
    "\n",
    "#ita for focal loss function\n",
    "ita=0\n",
    "\n",
    "# parameters in paper ita=2, epoch=50, batch size=512, CNN_kernel_size=(11,5,3), CNN_kernel_num=(16,32,64), CNN_strides=(3,1,1), \n",
    "# max_pool=(3,2), final_relu=64\n",
    "# parameters of mine ita=, epoch=50, batch size=512, CNN_kernel_size=(6,11,4), CNN_kernel_num=(32,32,64), CNN_strides=(2,1,1), \n",
    "# max_pool=(3,2), final_relu=16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.one_hot(K.argmax(y_pred, axis=-1), num_classes=p)\n",
    "    y_true = K.cast(y_true, y_pred.dtype)\n",
    "\n",
    "    tp = K.sum(y_true * y_pred, axis=0)\n",
    "    tn = K.sum((1 - y_true) * (1 - y_pred), axis=0)\n",
    "    fp = K.sum((1 - y_true) * y_pred, axis=0)\n",
    "    fn = K.sum(y_true * (1 - y_pred), axis=0)\n",
    "\n",
    "    precision = tp / (tp + fp + K.epsilon())\n",
    "    recall = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    return K.mean(2 * precision * recall / (precision + recall + K.epsilon()))\n",
    "\n",
    "\n",
    "def categorical_focal_loss(gamma=ita):\n",
    "    \"\"\"\n",
    "        Categorical form of focal loss.\n",
    "            FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n",
    "        References:\n",
    "            https://arxiv.org/pdf/1708.02002.pdf\n",
    "        Usage:\n",
    "            model.compile(loss=categorical_focal_loss(gamma=2), optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "            model.fit(class_weight={0:alpha0, 1:alpha1, ...}, ...)\n",
    "        Notes:\n",
    "           1. The alpha variable is the class_weight of keras.fit, so in implementation of the focal loss function\n",
    "           we needn't define this variable.\n",
    "           2. (important!!!) The output of the loss is the loss value of each training sample, not the total or average\n",
    "            loss of each batch.\n",
    "    \"\"\"\n",
    "\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        y_true = K.cast(y_true, y_pred.dtype)\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "\n",
    "        return K.sum(K.sum(-y_true * K.pow(1 - y_pred, gamma) * K.log(y_pred), axis=-1))\n",
    "\n",
    "    return focal_loss\n",
    "\n",
    "\n",
    "def create_model(l=0.0):\n",
    "    inputs1 = Input(shape=(shp, 1))\n",
    "    x1 = inputs1\n",
    "\n",
    "    x1 = Conv1D(16, kernel_size=8, strides=2, kernel_initializer=\"he_normal\", kernel_regularizer=l2(l),\n",
    "                bias_regularizer=l2(l))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Activation(\"relu\")(x1)\n",
    "\n",
    "    x1 = MaxPooling1D(3, strides=2)(x1)\n",
    "\n",
    "    x1 = Conv1D(32, kernel_size=5, kernel_initializer=\"he_normal\", kernel_regularizer=l2(l),\n",
    "                bias_regularizer=l2(l))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Activation(\"relu\")(x1)\n",
    "\n",
    "    x1 = MaxPooling1D(3, strides=2)(x1)\n",
    "\n",
    "    x1 = Conv1D(64, kernel_size=3, kernel_initializer=\"he_normal\", kernel_regularizer=l2(l),\n",
    "                bias_regularizer=l2(l))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Activation(\"relu\")(x1)\n",
    "\n",
    "    x1 = MaxPooling1D(3, strides=2)(x1)\n",
    "\n",
    "    x1 = Flatten()(x1)\n",
    "\n",
    "    inputs2 = Input(shape=(4,))\n",
    "    x2 = inputs2\n",
    "\n",
    "    x = Concatenate()([x1, x2])\n",
    "\n",
    "    x = Dense(64, kernel_initializer=\"he_normal\", kernel_regularizer=l2(l), bias_regularizer=l2(l),\n",
    "              activation=\"relu\")(x)\n",
    "\n",
    "    outputs = Dense(p, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=(inputs1, inputs2), outputs=outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "class MyGenerator(keras.utils.data_utils.Sequence):\n",
    "\n",
    "    def __init__(self, x1, x2, y, batch_size):\n",
    "        self.x1 = x1\n",
    "        self.x2 = x2\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(len(self.x1))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x1) // self.batch_size + 1\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        x1_batch = self.x1[self.indices[item * self.batch_size:(item + 1) * self.batch_size]]\n",
    "        x2_batch = self.x2[self.indices[item * self.batch_size:(item + 1) * self.batch_size]]\n",
    "        y_batch = self.y[self.indices[item * self.batch_size:(item + 1) * self.batch_size]]\n",
    "        return (x1_batch, x2_batch), y_batch\n",
    "\n",
    "\n",
    "def load_data(filename):\n",
    "    #load pkl file\n",
    "    import pickle\n",
    "\n",
    "    with open(filename, \"rb\") as f:\n",
    "        (x1_train, x2_train, y_train), (x1_test, x2_test, y_test) = pickle.load(f)\n",
    "\n",
    "    return (x1_train, x2_train, y_train), (x1_test, x2_test, y_test)\n",
    "\n",
    "\n",
    "def main(filename):\n",
    "    epochs = 50\n",
    "    batch_size = 512\n",
    "\n",
    "    # loading data\n",
    "    (x1_train, x2_train, y_train), (x1_test, x2_test, y_test) = load_data(filename)\n",
    "\n",
    "    x1_train = np.expand_dims(x1_train, axis=-1)\n",
    "    x1_test = np.expand_dims(x1_test, axis=-1)\n",
    "\n",
    "    scaler = RobustScaler()\n",
    "    x2_train = scaler.fit_transform(x2_train)\n",
    "    x2_test = scaler.transform(x2_test)\n",
    "\n",
    "    y_train = keras.utils.np_utils.to_categorical(y_train, num_classes=p)\n",
    "    y_test = keras.utils.np_utils.to_categorical(y_test, num_classes=p)\n",
    "\n",
    "    train_generator = MyGenerator(x1_train, x2_train, y_train, batch_size)\n",
    "    test_generator = MyGenerator(x1_test, x2_test, y_test, batch_size)\n",
    "\n",
    "    model = create_model(l=1e-3)\n",
    "\n",
    "    # callbacks\n",
    "    log_dir = os.path.join(\"./logs\", datetime.now().strftime(\"%H-%M-%S\"))\n",
    "    tb_cb = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "    def schedule(epoch, lr):\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            lr *= 0.1\n",
    "        return lr\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(schedule=schedule, verbose=0)\n",
    "\n",
    "    # training\n",
    "    model.compile(loss=categorical_focal_loss(gamma=ita), optimizer=\"adam\", metrics=[\"acc\", f1])\n",
    "    model.fit(train_generator, epochs=epochs, verbose=1, callbacks=[tb_cb, lr_scheduler],\n",
    "              validation_data=test_generator)\n",
    "\n",
    "    model.save(os.path.join(\"./models\", \"model_crossentropy.h5\"))\n",
    "    #model_focalloss(2)\n",
    "\n",
    "    #result(train)\n",
    "    y_true = np.argmax(y_train, axis=-1)\n",
    "    y_pred = np.argmax(model.predict([x1_train, x2_train], batch_size=batch_size, verbose=1), axis=-1)\n",
    "\n",
    "    print(\"training data\")\n",
    "    print(\"row:true, column:pred\")\n",
    "    print(pd.DataFrame(confusion_matrix(y_true, y_pred)))\n",
    "    print(\"classification_report\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "    #result(test)\n",
    "    y_true = np.argmax(y_test, axis=-1)\n",
    "    y_pred = np.argmax(model.predict([x1_test, x2_test], batch_size=batch_size, verbose=1), axis=-1)\n",
    "\n",
    "    print(\"testing data\")\n",
    "    print(\"row:true, column:pred\")\n",
    "    print(pd.DataFrame(confusion_matrix(y_true, y_pred)))\n",
    "    print(\"classification_report\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x1_train, x2_train, y_train), (x1_test, x2_test, y_test)=load_data(os.path.join(link2, \"CNNMPFL_preprocess.pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train labels: [49120.   339.]\n",
      "test labels: [12304.    74.]\n"
     ]
    }
   ],
   "source": [
    "x1_train = np.expand_dims(x1_train, axis=-1)\n",
    "x1_test = np.expand_dims(x1_test, axis=-1)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "x2_train = scaler.fit_transform(x2_train)\n",
    "x2_test = scaler.transform(x2_test)\n",
    "\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes=p)\n",
    "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes=p)\n",
    "\n",
    "print(\"train labels:\", np.sum(y_train, axis=0))\n",
    "print(\"test labels:\", np.sum(y_test, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 140, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 67, 16)       144         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 67, 16)       64          conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 67, 16)       0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 33, 16)       0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 29, 32)       2592        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 29, 32)       128         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 29, 32)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 14, 32)       0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 12, 64)       6208        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 12, 64)       256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 12, 64)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 5, 64)        0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 320)          0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 324)          0           flatten[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           20800       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            130         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 30,322\n",
      "Trainable params: 30,098\n",
      "Non-trainable params: 224\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(l=1e-3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "97/97 [==============================] - 5s 37ms/step - loss: 60.3062 - acc: 0.9781 - f1: 0.5404 - val_loss: 28.3763 - val_acc: 0.9885 - val_f1: 0.6501\n",
      "Epoch 2/50\n",
      "97/97 [==============================] - 3s 35ms/step - loss: 15.0753 - acc: 0.9939 - f1: 0.6940 - val_loss: 13.2407 - val_acc: 0.9943 - val_f1: 0.7169\n",
      "Epoch 3/50\n",
      "97/97 [==============================] - 3s 36ms/step - loss: 11.0149 - acc: 0.9948 - f1: 0.7392 - val_loss: 9.7113 - val_acc: 0.9955 - val_f1: 0.7409\n",
      "Epoch 4/50\n",
      "97/97 [==============================] - 3s 36ms/step - loss: 9.3701 - acc: 0.9955 - f1: 0.7711 - val_loss: 15.5332 - val_acc: 0.9943 - val_f1: 0.7212\n",
      "Epoch 5/50\n",
      "97/97 [==============================] - 3s 35ms/step - loss: 9.9496 - acc: 0.9956 - f1: 0.7951 - val_loss: 8.6415 - val_acc: 0.9956 - val_f1: 0.7206\n",
      "Epoch 6/50\n",
      "97/97 [==============================] - 3s 35ms/step - loss: 7.8747 - acc: 0.9959 - f1: 0.8209 - val_loss: 8.3620 - val_acc: 0.9956 - val_f1: 0.7518\n",
      "Epoch 7/50\n",
      "97/97 [==============================] - 3s 35ms/step - loss: 7.6885 - acc: 0.9964 - f1: 0.7955 - val_loss: 9.7440 - val_acc: 0.9962 - val_f1: 0.7844\n",
      "Epoch 8/50\n",
      "97/97 [==============================] - 3s 34ms/step - loss: 6.9506 - acc: 0.9964 - f1: 0.8181 - val_loss: 9.4559 - val_acc: 0.9952 - val_f1: 0.7284\n",
      "Epoch 9/50\n",
      "97/97 [==============================] - 3s 35ms/step - loss: 6.5354 - acc: 0.9966 - f1: 0.8198 - val_loss: 8.2765 - val_acc: 0.9959 - val_f1: 0.7225\n",
      "Epoch 10/50\n",
      "97/97 [==============================] - 3s 35ms/step - loss: 5.6003 - acc: 0.9972 - f1: 0.8582 - val_loss: 7.9208 - val_acc: 0.9962 - val_f1: 0.7930\n",
      "Epoch 11/50\n",
      "97/97 [==============================] - 3s 35ms/step - loss: 5.5042 - acc: 0.9973 - f1: 0.8694 - val_loss: 7.8685 - val_acc: 0.9959 - val_f1: 0.7529\n",
      "Epoch 12/50\n",
      "97/97 [==============================] - 3s 35ms/step - loss: 5.2067 - acc: 0.9977 - f1: 0.8856 - val_loss: 8.0257 - val_acc: 0.9962 - val_f1: 0.7634\n",
      "Epoch 13/50\n",
      "97/97 [==============================] - 3s 35ms/step - loss: 5.1784 - acc: 0.9976 - f1: 0.8810 - val_loss: 7.8481 - val_acc: 0.9963 - val_f1: 0.7527\n",
      "Epoch 14/50\n",
      "97/97 [==============================] - 3s 36ms/step - loss: 5.1388 - acc: 0.9975 - f1: 0.8859 - val_loss: 7.9260 - val_acc: 0.9960 - val_f1: 0.7881\n",
      "Epoch 15/50\n",
      "97/97 [==============================] - 4s 37ms/step - loss: 4.9011 - acc: 0.9976 - f1: 0.8866 - val_loss: 8.0128 - val_acc: 0.9961 - val_f1: 0.7619\n",
      "Epoch 16/50\n",
      "97/97 [==============================] - 3s 36ms/step - loss: 4.9221 - acc: 0.9976 - f1: 0.8692 - val_loss: 7.9665 - val_acc: 0.9960 - val_f1: 0.7528\n",
      "Epoch 17/50\n",
      "97/97 [==============================] - 3s 36ms/step - loss: 5.0721 - acc: 0.9977 - f1: 0.8920 - val_loss: 8.0733 - val_acc: 0.9959 - val_f1: 0.7438\n",
      "Epoch 18/50\n",
      "97/97 [==============================] - 4s 36ms/step - loss: 4.8893 - acc: 0.9975 - f1: 0.8769 - val_loss: 7.8798 - val_acc: 0.9959 - val_f1: 0.7265\n",
      "Epoch 19/50\n",
      "97/97 [==============================] - 4s 36ms/step - loss: 4.8238 - acc: 0.9978 - f1: 0.8966 - val_loss: 8.0496 - val_acc: 0.9959 - val_f1: 0.7749\n",
      "Epoch 20/50\n",
      "97/97 [==============================] - 3s 35ms/step - loss: 4.9442 - acc: 0.9975 - f1: 0.8601 - val_loss: 8.0914 - val_acc: 0.9958 - val_f1: 0.8059\n",
      "Epoch 21/50\n",
      "97/97 [==============================] - 3s 36ms/step - loss: 4.5553 - acc: 0.9980 - f1: 0.9027 - val_loss: 8.1140 - val_acc: 0.9958 - val_f1: 0.7420\n",
      "Epoch 22/50\n",
      "97/97 [==============================] - 3s 36ms/step - loss: 4.6236 - acc: 0.9977 - f1: 0.8612 - val_loss: 8.1194 - val_acc: 0.9958 - val_f1: 0.7624\n",
      "Epoch 23/50\n",
      "97/97 [==============================] - 4s 36ms/step - loss: 4.7409 - acc: 0.9979 - f1: 0.8975 - val_loss: 8.0987 - val_acc: 0.9960 - val_f1: 0.7358\n",
      "Epoch 24/50\n",
      "97/97 [==============================] - 3s 35ms/step - loss: 4.6890 - acc: 0.9978 - f1: 0.8804 - val_loss: 8.0900 - val_acc: 0.9959 - val_f1: 0.7787\n",
      "Epoch 25/50\n",
      "97/97 [==============================] - 4s 36ms/step - loss: 4.6274 - acc: 0.9978 - f1: 0.8780 - val_loss: 8.1118 - val_acc: 0.9959 - val_f1: 0.7554\n",
      "Epoch 26/50\n",
      "97/97 [==============================] - 3s 36ms/step - loss: 4.7344 - acc: 0.9977 - f1: 0.8843 - val_loss: 8.1263 - val_acc: 0.9959 - val_f1: 0.7434\n",
      "Epoch 27/50\n",
      "97/97 [==============================] - 4s 36ms/step - loss: 4.5862 - acc: 0.9979 - f1: 0.9022 - val_loss: 8.1065 - val_acc: 0.9959 - val_f1: 0.7539\n",
      "Epoch 28/50\n",
      "97/97 [==============================] - 3s 36ms/step - loss: 4.6947 - acc: 0.9978 - f1: 0.8853 - val_loss: 8.1063 - val_acc: 0.9959 - val_f1: 0.7552\n",
      "Epoch 29/50\n",
      "97/97 [==============================] - 3s 36ms/step - loss: 4.6185 - acc: 0.9979 - f1: 0.8901 - val_loss: 8.0750 - val_acc: 0.9958 - val_f1: 0.7592\n",
      "Epoch 30/50\n",
      "97/97 [==============================] - 3s 35ms/step - loss: 4.6377 - acc: 0.9979 - f1: 0.8900 - val_loss: 8.0800 - val_acc: 0.9958 - val_f1: 0.7331\n",
      "Epoch 31/50\n",
      "97/97 [==============================] - 3s 36ms/step - loss: 4.7113 - acc: 0.9978 - f1: 0.8831 - val_loss: 8.0762 - val_acc: 0.9958 - val_f1: 0.7791\n",
      "Epoch 32/50\n",
      "97/97 [==============================] - 4s 37ms/step - loss: 4.7234 - acc: 0.9980 - f1: 0.8940 - val_loss: 8.1107 - val_acc: 0.9958 - val_f1: 0.7540\n",
      "Epoch 33/50\n",
      "97/97 [==============================] - 3s 36ms/step - loss: 4.5575 - acc: 0.9981 - f1: 0.8959 - val_loss: 8.1226 - val_acc: 0.9958 - val_f1: 0.7635\n",
      "Epoch 34/50\n",
      "97/97 [==============================] - 3s 36ms/step - loss: 4.5843 - acc: 0.9979 - f1: 0.8946 - val_loss: 8.0766 - val_acc: 0.9958 - val_f1: 0.7242\n",
      "Epoch 35/50\n",
      "97/97 [==============================] - 3s 35ms/step - loss: 4.5087 - acc: 0.9979 - f1: 0.8956 - val_loss: 8.1171 - val_acc: 0.9958 - val_f1: 0.7499\n",
      "Epoch 36/50\n",
      "97/97 [==============================] - 4s 36ms/step - loss: 4.7398 - acc: 0.9978 - f1: 0.9031 - val_loss: 8.1341 - val_acc: 0.9958 - val_f1: 0.7362\n",
      "Epoch 37/50\n",
      "97/97 [==============================] - 4s 36ms/step - loss: 4.6543 - acc: 0.9978 - f1: 0.8611 - val_loss: 8.1566 - val_acc: 0.9958 - val_f1: 0.7549\n",
      "Epoch 38/50\n",
      "97/97 [==============================] - 3s 36ms/step - loss: 4.5106 - acc: 0.9979 - f1: 0.8991 - val_loss: 8.0488 - val_acc: 0.9959 - val_f1: 0.7535\n",
      "Epoch 39/50\n",
      "97/97 [==============================] - 3s 35ms/step - loss: 4.6185 - acc: 0.9979 - f1: 0.8902 - val_loss: 8.1414 - val_acc: 0.9958 - val_f1: 0.7435\n",
      "Epoch 40/50\n",
      "97/97 [==============================] - 3s 35ms/step - loss: 4.8892 - acc: 0.9977 - f1: 0.8870 - val_loss: 8.1212 - val_acc: 0.9958 - val_f1: 0.7510\n",
      "Epoch 41/50\n",
      "97/97 [==============================] - 3s 36ms/step - loss: 4.6908 - acc: 0.9980 - f1: 0.8939 - val_loss: 8.1356 - val_acc: 0.9958 - val_f1: 0.7516\n",
      "Epoch 42/50\n",
      "97/97 [==============================] - 3s 35ms/step - loss: 4.7320 - acc: 0.9978 - f1: 0.8950 - val_loss: 8.1095 - val_acc: 0.9958 - val_f1: 0.7641\n",
      "Epoch 43/50\n",
      "97/97 [==============================] - 3s 35ms/step - loss: 4.6827 - acc: 0.9979 - f1: 0.9017 - val_loss: 8.1002 - val_acc: 0.9958 - val_f1: 0.7623\n",
      "Epoch 44/50\n",
      "97/97 [==============================] - 3s 35ms/step - loss: 4.5989 - acc: 0.9979 - f1: 0.8961 - val_loss: 8.1412 - val_acc: 0.9958 - val_f1: 0.7769\n",
      "Epoch 45/50\n",
      "97/97 [==============================] - 4s 36ms/step - loss: 4.6728 - acc: 0.9980 - f1: 0.9030 - val_loss: 8.1534 - val_acc: 0.9958 - val_f1: 0.7387\n",
      "Epoch 46/50\n",
      "97/97 [==============================] - 4s 38ms/step - loss: 4.6655 - acc: 0.9980 - f1: 0.9025 - val_loss: 8.0896 - val_acc: 0.9958 - val_f1: 0.7611\n",
      "Epoch 47/50\n",
      "97/97 [==============================] - 4s 36ms/step - loss: 4.5937 - acc: 0.9980 - f1: 0.8946 - val_loss: 8.0495 - val_acc: 0.9959 - val_f1: 0.7733\n",
      "Epoch 48/50\n",
      "97/97 [==============================] - 3s 35ms/step - loss: 4.7492 - acc: 0.9977 - f1: 0.8878 - val_loss: 8.1546 - val_acc: 0.9958 - val_f1: 0.7584\n",
      "Epoch 49/50\n",
      "97/97 [==============================] - 4s 36ms/step - loss: 4.6672 - acc: 0.9979 - f1: 0.8902 - val_loss: 8.1457 - val_acc: 0.9958 - val_f1: 0.7577\n",
      "Epoch 50/50\n",
      "97/97 [==============================] - 4s 37ms/step - loss: 4.6717 - acc: 0.9978 - f1: 0.8836 - val_loss: 8.1373 - val_acc: 0.9958 - val_f1: 0.7432\n",
      "97/97 [==============================] - 1s 8ms/step\n",
      "training data\n",
      "row:true, column:pred\n",
      "       0    1\n",
      "0  49110   10\n",
      "1    101  238\n",
      "classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9979    0.9998    0.9989     49120\n",
      "           1     0.9597    0.7021    0.8109       339\n",
      "\n",
      "    accuracy                         0.9978     49459\n",
      "   macro avg     0.9788    0.8509    0.9049     49459\n",
      "weighted avg     0.9977    0.9978    0.9976     49459\n",
      "\n",
      "25/25 [==============================] - 0s 8ms/step\n",
      "testing data\n",
      "row:true, column:pred\n",
      "       0   1\n",
      "0  12288  16\n",
      "1     36  38\n",
      "classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9971    0.9987    0.9979     12304\n",
      "           1     0.7037    0.5135    0.5938        74\n",
      "\n",
      "    accuracy                         0.9958     12378\n",
      "   macro avg     0.8504    0.7561    0.7958     12378\n",
      "weighted avg     0.9953    0.9958    0.9955     12378\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def seed_every(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "seed_every(61926)\n",
    "CNNMPFL_modeling=main(os.path.join(link2, \"CNNMPFL_preprocess.pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg-focalloss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
