{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------Pre_training--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python 3.6.13\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_classes\n",
    "p=2\n",
    "#shape\n",
    "shp=140\n",
    "#wtraining data & testing data link\n",
    "link=\"C:/Users/ASUS/Desktop/cg working/HRV/Identify correct wave methods/Deep-Neural-Network-For-Heartbeat-Classification-master/models/\"\n",
    "\n",
    "#ita for focal loss function\n",
    "ita=0\n",
    "\n",
    "# parameters in paper ita=2, epoch=50, batch size=512, CNN_kernel_size=(11,5,3), CNN_kernel_num=(16,32,64), CNN_strides=(3,1,1), \n",
    "# max_pool=(3,2), final_relu=64\n",
    "# parameters of mine ita=, epoch=50, batch size=512, CNN_kernel_size=(6,11,4), CNN_kernel_num=(32,32,64), CNN_strides=(2,1,1), \n",
    "# max_pool=(3,2), final_relu=16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre_training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.one_hot(K.argmax(y_pred, axis=-1), num_classes=p)\n",
    "    y_true = K.cast(y_true, y_pred.dtype)\n",
    "\n",
    "    tp = K.sum(y_true * y_pred, axis=0)\n",
    "    tn = K.sum((1 - y_true) * (1 - y_pred), axis=0)\n",
    "    fp = K.sum((1 - y_true) * y_pred, axis=0)\n",
    "    fn = K.sum(y_true * (1 - y_pred), axis=0)\n",
    "\n",
    "    precision = tp / (tp + fp + K.epsilon())\n",
    "    recall = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    return K.mean(2 * precision * recall / (precision + recall + K.epsilon()))\n",
    "\n",
    "\n",
    "def categorical_focal_loss(gamma=ita):\n",
    "    \"\"\"\n",
    "        Categorical form of focal loss.\n",
    "            FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n",
    "        References:\n",
    "            https://arxiv.org/pdf/1708.02002.pdf\n",
    "        Usage:\n",
    "            model.compile(loss=categorical_focal_loss(gamma=2), optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "            model.fit(class_weight={0:alpha0, 1:alpha1, ...}, ...)\n",
    "        Notes:\n",
    "           1. The alpha variable is the class_weight of keras.fit, so in implementation of the focal loss function\n",
    "           we needn't define this variable.\n",
    "           2. (important!!!) The output of the loss is the loss value of each training sample, not the total or average\n",
    "            loss of each batch.\n",
    "    \"\"\"\n",
    "\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        y_true = K.cast(y_true, y_pred.dtype)\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "\n",
    "        return K.sum(K.sum(-y_true * K.pow(1 - y_pred, gamma) * K.log(y_pred), axis=-1))\n",
    "\n",
    "    return focal_loss\n",
    "\n",
    "\n",
    "def load_data(filename):\n",
    "    #load pkl file\n",
    "    import pickle\n",
    "\n",
    "    with open(filename, \"rb\") as f:\n",
    "        (x1_train, x2_train, y_train), (x1_test, x2_test, y_test) = pickle.load(f)\n",
    "\n",
    "    return (x1_train, x2_train, y_train), (x1_test, x2_test, y_test)\n",
    "\n",
    "\n",
    "def pre_training(filename):\n",
    "    batch_size = 512\n",
    "\n",
    "    # loading data\n",
    "    (x1_train, x2_train, y_train), (x1_test, x2_test, y_test) = load_data(filename)\n",
    "\n",
    "    x1_train = np.expand_dims(x1_train, axis=-1)\n",
    "    x1_test = np.expand_dims(x1_test, axis=-1)\n",
    "\n",
    "    scaler = RobustScaler()\n",
    "    x2_train = scaler.fit_transform(x2_train)\n",
    "    x2_test = scaler.transform(x2_test)\n",
    "\n",
    "    y_train = keras.utils.np_utils.to_categorical(y_train, num_classes=p)\n",
    "    y_test = keras.utils.np_utils.to_categorical(y_test, num_classes=p)\n",
    "\n",
    "    model = load_model(os.path.join(link, \"model_crossentropy.h5\"),\n",
    "                       custom_objects={\"focal_loss\": categorical_focal_loss(gamma=ita),\n",
    "                                       \"f1\": f1})\n",
    "    model.summary()\n",
    "\n",
    "    #result(train)\n",
    "    print(\"training:\")\n",
    "    y_true = np.argmax(y_train, axis=-1)\n",
    "    y_pred = np.argmax(model.predict([x1_train, x2_train], batch_size=batch_size, verbose=1), axis=-1)\n",
    "\n",
    "    print(\"row:true, column:pred\")\n",
    "    print(pd.DataFrame(confusion_matrix(y_true, y_pred)))\n",
    "    print(\"classification_report\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "    #result(test)\n",
    "    print(\"testing:\")\n",
    "    y_true = np.argmax(y_test, axis=-1)\n",
    "    y_pred = np.argmax(model.predict([x1_test, x2_test], batch_size=batch_size, verbose=1), axis=-1)\n",
    "\n",
    "    print(\"row:true, column:pred\")\n",
    "    print(pd.DataFrame(confusion_matrix(y_true, y_pred)))\n",
    "    print(\"classification_report\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict new wave (marked data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 140, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 67, 16)       144         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 67, 16)       64          conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 67, 16)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 33, 16)       0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 29, 32)       2592        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 29, 32)       128         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 29, 32)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 14, 32)       0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 12, 64)       6208        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 12, 64)       256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 12, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 5, 64)        0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 320)          0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 324)          0           flatten_1[0][0]                  \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           20800       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            130         dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 30,322\n",
      "Trainable params: 30,098\n",
      "Non-trainable params: 224\n",
      "__________________________________________________________________________________________________\n",
      "training:\n",
      "97/97 [==============================] - 1s 8ms/step\n",
      "row:true, column:pred\n",
      "       0    1\n",
      "0  49110   10\n",
      "1    101  238\n",
      "classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9979    0.9998    0.9989     49120\n",
      "           1     0.9597    0.7021    0.8109       339\n",
      "\n",
      "    accuracy                         0.9978     49459\n",
      "   macro avg     0.9788    0.8509    0.9049     49459\n",
      "weighted avg     0.9977    0.9978    0.9976     49459\n",
      "\n",
      "testing:\n",
      "25/25 [==============================] - 0s 6ms/step\n",
      "row:true, column:pred\n",
      "       0   1\n",
      "0  12288  16\n",
      "1     36  38\n",
      "classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9971    0.9987    0.9979     12304\n",
      "           1     0.7037    0.5135    0.5938        74\n",
      "\n",
      "    accuracy                         0.9958     12378\n",
      "   macro avg     0.8504    0.7561    0.7958     12378\n",
      "weighted avg     0.9953    0.9958    0.9955     12378\n",
      "\n"
     ]
    }
   ],
   "source": [
    "link2=\"C:/Users/ASUS/Desktop/cg working/HRV/Identify correct wave methods/Deep-Neural-Network-For-Heartbeat-Classification-master/\"\n",
    "CNNMPFL_modeling=pre_training(os.path.join(link2, \"CNNMPFL_preprocess.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict new wave (unmarked data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 140, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 67, 16)       144         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 67, 16)       64          conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 67, 16)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 33, 16)       0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 29, 32)       2592        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 29, 32)       128         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 29, 32)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 14, 32)       0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 12, 64)       6208        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 12, 64)       256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 12, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 5, 64)        0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 320)          0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 324)          0           flatten_1[0][0]                  \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           20800       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            130         dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 30,322\n",
      "Trainable params: 30,098\n",
      "Non-trainable params: 224\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def load_data(filename):\n",
    "    #load pkl file\n",
    "    import pickle\n",
    "\n",
    "    with open(filename, \"rb\") as f:\n",
    "        (x1_train, x2_train), (x1_test, x2_test) = pickle.load(f)\n",
    "\n",
    "    return (x1_train, x2_train), (x1_test, x2_test)\n",
    "\n",
    "batch_size = 512\n",
    "# loading data\n",
    "(x1_train, x2_train), (x1_test, x2_test) = load_data(os.path.join(link2, \"CNNMPFL_newwave_preprocess.pkl\"))\n",
    "\n",
    "x1_train = np.expand_dims(x1_train, axis=-1)\n",
    "x1_test = np.expand_dims(x1_test, axis=-1)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "x2_train = scaler.fit_transform(x2_train)\n",
    "x2_test = scaler.transform(x2_test)\n",
    "\n",
    "model = load_model(os.path.join(link, \"model_crossentropy.h5\"),\n",
    "                       custom_objects={\"focal_loss\": categorical_focal_loss(gamma=ita),\n",
    "                                       \"f1\": f1})\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training:\n",
      "3/3 [==============================] - 0s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#result(train)\n",
    "print(\"training:\")\n",
    "y_pred = np.argmax(model.predict([x1_train, x2_train], batch_size=batch_size, verbose=1), axis=-1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classfication</th>\n",
       "      <th>Normal</th>\n",
       "      <th>Abnormal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>1141.000000</td>\n",
       "      <td>91.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>prop.</td>\n",
       "      <td>0.926136</td>\n",
       "      <td>0.073864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  classfication       Normal   Abnormal\n",
       "0         count  1141.000000  91.000000\n",
       "1         prop.     0.926136   0.073864"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table={\"classfication\":[\"count\",\"prop.\"],\n",
    "       \"Normal\":[len(y_pred)-sum(y_pred),1-sum(y_pred)/len(y_pred)],\n",
    "       \"Abnormal\":[sum(y_pred),sum(y_pred)/len(y_pred)]}\n",
    "table=pd.DataFrame(table)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing:\n",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#result(test)\n",
    "print(\"testing:\")\n",
    "y_pred = np.argmax(model.predict([x1_test, x2_test], batch_size=batch_size, verbose=1), axis=-1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classfication</th>\n",
       "      <th>Normal</th>\n",
       "      <th>Abnormal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>283.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>prop.</td>\n",
       "      <td>0.918831</td>\n",
       "      <td>0.081169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  classfication      Normal   Abnormal\n",
       "0         count  283.000000  25.000000\n",
       "1         prop.    0.918831   0.081169"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table={\"classfication\":[\"count\",\"prop.\"],\n",
    "       \"Normal\":[len(y_pred)-sum(y_pred),1-sum(y_pred)/len(y_pred)],\n",
    "       \"Abnormal\":[sum(y_pred),sum(y_pred)/len(y_pred)]}\n",
    "table=pd.DataFrame(table)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg-focalloss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
